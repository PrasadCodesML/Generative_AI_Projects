{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1530,
     "status": "ok",
     "timestamp": 1739875206285,
     "user": {
      "displayName": "Prasad Khambadkar",
      "userId": "03733107158190389699"
     },
     "user_tz": -330
    },
    "id": "QDgf849QRW0B"
   },
   "outputs": [],
   "source": [
    "# Language Models -\n",
    "# 1. LLMs\n",
    "# 2. Chat Models\n",
    "\n",
    "# LLMs -> are general purpose you can use for any type of NLP application, you cannot assign roles (use if you want to make text generation, summarization, translation, creative writing, code generation)\n",
    "# LLMs -> GPT-3, Llama-2-7b, OPT-1.3B\n",
    "\n",
    "# Chat Models -> specialzed for conversations, you can assign role to chat model (use if you want to make conversational ai, chatbots, virtual assistant, customer support, ai tutor)\n",
    "# Chat Models -> GPT-4, Claude, GPT-3.5-turbo, Llama-2-Chat, Mistral-Instruct\n",
    "\n",
    "# temperature -> parameter that controls the randomness of the language model's output.\n",
    "# A higher temperature will result in more diverse and creative output, while a lower temperature will result in more conservative and deterministic output.\n",
    "# When temperature is low then for same input you will get almost similar output but when temperature is high for same input you will get more different output\n",
    "# Factual ans(math, code, facts) -> 0.0 to 0.3\n",
    "# Balanced ans(general qa, explaination) -> 0.5 to 0.7\n",
    "# Creativity ans(creative writing, storytelling, jokes) -> 0.9 to 1.2\n",
    "# Maximum randomness(wild ideas, brainstorming) -> 1.5+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_PAX7_7pV82G"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3470,
     "status": "ok",
     "timestamp": 1739875211630,
     "user": {
      "displayName": "Prasad Khambadkar",
      "userId": "03733107158190389699"
     },
     "user_tz": -330
    },
    "id": "1M_LiNMCUm0S",
    "outputId": "911a28ad-1a97-4667-da31-5121f332840b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/54.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m51.2/54.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain-core google-generativeai langchain-openai --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Oo354vxTsQ0"
   },
   "source": [
    "# LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 620,
     "status": "ok",
     "timestamp": 1739875214065,
     "user": {
      "displayName": "Prasad Khambadkar",
      "userId": "03733107158190389699"
     },
     "user_tz": -330
    },
    "id": "LbF6Im_RS1O9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import python_dotenv\n",
    "\n",
    "# Load environment variables from.env file\n",
    "\n",
    "python_dotenv.load_dotenv()\n",
    "\n",
    "# Load OpenAI API key\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JlxbopJBUMwi"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model='gpt-3.5-turbo-instruct')\n",
    "result = llm.invoke(\"What is the capital of India\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Dfs_lIEVxnW"
   },
   "source": [
    "# Chat Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5swdghOaUjz7"
   },
   "outputs": [],
   "source": [
    "# Openai chat models\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model='gpt-4')\n",
    "result = model.invoke(\"What is the capital of India\", temperature=1, max_completion_tokens=10)\n",
    "print(result)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fuOfd5vUpXMU"
   },
   "outputs": [],
   "source": [
    "!pip install langchain_anthropic --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i7gzk-U-XjKh"
   },
   "outputs": [],
   "source": [
    "# Athropic chat models\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "model = ChatAnthropic(model='claude-2')\n",
    "result = model.invoke(\"What is the capital of India\")\n",
    "print(result)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MnjFQKTqptH5"
   },
   "outputs": [],
   "source": [
    "!pip install langchain_google_genai --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NSEkz_mcZFw3"
   },
   "outputs": [],
   "source": [
    "# Google chat models\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model='gemini-1.5-pro')\n",
    "result = model.invoke(\"What is the capital of India\")\n",
    "print(result)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "aborted",
     "timestamp": 1739875226272,
     "user": {
      "displayName": "Prasad Khambadkar",
      "userId": "03733107158190389699"
     },
     "user_tz": -330
    },
    "id": "kV-95YAlZytZ"
   },
   "outputs": [],
   "source": [
    "# # Open source Models\n",
    "# LlaMA-2-7B/13B/70B -> General purpose text generation\n",
    "# Mixtral-8x7B -> Efficient and fast responses\n",
    "# Mixtral-7B -> Best small-scale model (outperforms LlaMA-2-12B)\n",
    "# Falcon-7B/40B -> high speed inference\n",
    "# BLOOM-176B -> Multilingual text generation\n",
    "# GPT-J-6B -> Lightweight and efficient\n",
    "# GPT-NeoX-20B -> Large-scale applications\n",
    "# StableLM -> Compact models for chatbots\n",
    "\n",
    "# Opensource ->\n",
    "# 1. Inference API\n",
    "# 2. Running Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 3980,
     "status": "ok",
     "timestamp": 1739875418871,
     "user": {
      "displayName": "Prasad Khambadkar",
      "userId": "03733107158190389699"
     },
     "user_tz": -330
    },
    "id": "sXbwQtTdcd6a"
   },
   "outputs": [],
   "source": [
    "!pip install huggingface_hub --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IGpLRlLheJl5"
   },
   "outputs": [],
   "source": [
    "!pip install langchain_huggingface --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1739875547978,
     "user": {
      "displayName": "Prasad Khambadkar",
      "userId": "03733107158190389699"
     },
     "user_tz": -330
    },
    "id": "qHF7NZc1fLl6"
   },
   "outputs": [],
   "source": [
    "os.environ[\"HUGGINGFACEHUB_ACCESS_TOKEN\"] = os.getenv(\"HUGGINGFACEHUB_ACCESS_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7iT3crBsdFGJ"
   },
   "outputs": [],
   "source": [
    "# Hugging Face API\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    task=\"text-generation\"\n",
    ")\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "result = model.invoke(\"What is the capital of India?\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PMPjCGNld7YM"
   },
   "outputs": [],
   "source": [
    "# From hugging face local\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs=dict(\n",
    "        temperature=0.5,\n",
    "        max_new_tokens=100\n",
    "    )\n",
    ")\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "result = model.invoke(\"What is the capital of India?\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3LtuhJgbrwgd"
   },
   "source": [
    "# Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QPknkVrlq6ol"
   },
   "outputs": [],
   "source": [
    "# Open AI Embeddings for query\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedding = OpenAIEmbeddings(model='text-embedding-3-large', dimensions=32)\n",
    "\n",
    "result = embedding.embed_query(\"Delhi is the capital of India\")\n",
    "\n",
    "print(str(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iwWGfW-Gv7Hz"
   },
   "outputs": [],
   "source": [
    "# Open AI Embeddings for docs\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedding = OpenAIEmbeddings(model='text-embedding-3-large', dimensions=32)\n",
    "\n",
    "documents = [\n",
    "    \"Delhi is the capital of India\",\n",
    "    \"Kolkata is the capital of West Bengal\",\n",
    "    \"Paris is the capital of France\"\n",
    "]\n",
    "\n",
    "result = embedding.embed_documents(documents)\n",
    "\n",
    "print(str(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i83Hh8AnwFI4"
   },
   "outputs": [],
   "source": [
    "# Hugging Face Embeddings local for query\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "documents = [\n",
    "    \"Delhi is the capital of India\",\n",
    "    \"Kolkata is the capital of West Bengal\",\n",
    "    \"Paris is the capital of France\"\n",
    "]\n",
    "\n",
    "vector = embedding.embed_documents(documents)\n",
    "\n",
    "print(str(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qOeO2gtqwQGf"
   },
   "outputs": [],
   "source": [
    "# Cosine Similarity : Open AI Embeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "embedding = OpenAIEmbeddings(model='text-embedding-3-large', dimensions=300)\n",
    "\n",
    "documents = [\n",
    "    \"Virat Kohli is an Indian cricketer known for his aggressive batting and leadership.\",\n",
    "    \"MS Dhoni is a former Indian captain famous for his calm demeanor and finishing skills.\",\n",
    "    \"Sachin Tendulkar, also known as the 'God of Cricket', holds many batting records.\",\n",
    "    \"Rohit Sharma is known for his elegant batting and record-breaking double centuries.\",\n",
    "    \"Jasprit Bumrah is an Indian fast bowler known for his unorthodox action and yorkers.\"\n",
    "]\n",
    "\n",
    "query = 'tell me about bumrah'\n",
    "\n",
    "doc_embeddings = embedding.embed_documents(documents)\n",
    "query_embedding = embedding.embed_query(query)\n",
    "\n",
    "scores = cosine_similarity([query_embedding], doc_embeddings)[0]\n",
    "\n",
    "index, score = sorted(list(enumerate(scores)),key=lambda x:x[1])[-1]\n",
    "\n",
    "print(query)\n",
    "print(documents[index])\n",
    "print(\"similarity score is:\", score)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM7VJVO89XZZesHTZ1Ne9Yp",
   "collapsed_sections": [
    "_PAX7_7pV82G",
    "8Oo354vxTsQ0",
    "1Dfs_lIEVxnW",
    "3LtuhJgbrwgd"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
